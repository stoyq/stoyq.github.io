[
  {
    "objectID": "posts/kendama-cup-scores.html",
    "href": "posts/kendama-cup-scores.html",
    "title": "Scores from the Kendama World Cup 2025",
    "section": "",
    "text": "My friend recently showed me the final scores from the Kendama World Cup 2025:\n\n\n\nWorld Cup 2025 Final Scores\n\n\nI decided to plot out the distribution of scores.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nscores = [\n    2043, 1622, 1457, 1448, 1403, 1395, 1371, 1347, 1285, 1277,\n    1206, 1205, 1159, 1113, 1063, 1062, 1008, 1000, 1000, 956,\n    948, 939, 938, 924, 917, 911, 910, 885, 872, 869, 820, 797,\n    772, 729, 703, 666, 665, 662, 578, 536, 485, 472, 452, 340,\n    306, 273, 82\n]\n\ndf = pd.DataFrame({'score': scores})\n\nplt.figure(figsize=(10,6))\nplt.hist(df['score'], bins=10, edgecolor='black')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.title('Kendama World Cup 2025 Scores Histogram')\nplt.show()\n\n\n\n\n\n\n\n\nTo my surprise the distribution looks bell-shaped, with a very obvious peak at 1000! So while the table form is useful to look up player names and scores, the histogram is even more insightful. I did not expect scores from a competition to also be distributed with such a familar shape. What steps can I take next to see if the scores are normally distributed? Can I apply Central Limit Theorem to make any inferences?"
  },
  {
    "objectID": "posts/detecting-ai-text.html",
    "href": "posts/detecting-ai-text.html",
    "title": "Searching for Authentic Writing in a World of AI-Generated Text",
    "section": "",
    "text": "A human using a magnifying glass to distinguish between human-written and AI-generated text. Image generated by ChatGPT.\nIn the past year, the use of generative AI tools such as ChatGPT has exploded in popularity. ChatGPT can be used to help people write emails, essays, summaries, and creative stories almost instantly. With more people using such tools, more people are wanting more authentic text written by real humans. So an important question has come up: can we distinguish between human-written and AI-generated text? There is plenty of research done in this area by (Fraser 2025; Georgiou 2025; Fiedler et al. 2025).\nThis project looks to answer that question using data science techniques. It is important to define the problem, because the term “AI-generated” means something different to different people. If a human uses AI to generate text (AI-assisted), does that still count as human-written? Should there be 3 different categories (human-written, AI-assisted, and AI-generated)? And if so, how much assistance can a human get from AI before the text is classified as AI-generated? For this project, we will keep it simple. We will compare text written completely by humans (unassisted by AI) with text written by AI (produced by generative tools such as ChatGPT)."
  },
  {
    "objectID": "posts/detecting-ai-text.html#collecting-the-dataset",
    "href": "posts/detecting-ai-text.html#collecting-the-dataset",
    "title": "Searching for Authentic Writing in a World of AI-Generated Text",
    "section": "Collecting the Dataset",
    "text": "Collecting the Dataset\nTo conduct this study, we will collect written samples from English speakers with different ages and educational backgrounds. Everyone will respond to the same prompt. Then we will also have ChatGPT respond to the same prompt. This way, we get two comparable sets of responses, and we know the ground truth.\nExamples of prompts might include questions such as:\n\n\n“What does a typical weekend look like for you?”\n“Describe a memorable experience from your childhood.”\n“What are your thoughts on working or studying remotely?”\n\n\nThese types of questions allow for variation in tone, structure, and vocabulary, while still keeping the topic consistent across participants.\nFor each prompt, we expect to see noticeable differences between human-written and AI-generated responses. Human responses may include personal anecdotes, informal language, minor grammatical inconsistencies, or abrupt shifts in thought.\nFor example, a human might write something like:\n\n“I try to keep my weekends as relaxed as possible. First I have to finish all my MDS assignments on Saturday. But I will make sure I do my 5km Parkrun, and have a nice breakfast somewhere. Sometimes I will also meet up with friends and watch a movie.”\n\nIn contrast, a response generated by ChatGPT to the same prompt might be more structured and polished, such as:\n\n“On weekends, many people take time to relax and recharge after a busy workweek. Common activities include completing household chores, spending time with friends, and engaging in hobbies that promote well-being.”\n\nWhile both answers address the same question, they differ in sentence structure and writing style. These differences form the basis of the features that will later be used for classification."
  },
  {
    "objectID": "posts/detecting-ai-text.html#the-dataset",
    "href": "posts/detecting-ai-text.html#the-dataset",
    "title": "Searching for Authentic Writing in a World of AI-Generated Text",
    "section": "The Dataset",
    "text": "The Dataset\nHere is an example of what the dataset might look like:\n| Question                                       | Response                                            | Target |\n|------------------------------------------------|-----------------------------------------------------|--------|\n| What does a typical weekend look like for you? | I try to keep my weekends as relaxed as possible... | Human  |\n| What does a typical weekend look like for you? | On weekends, many people take time to relax...      | AI     |\n| Describe a memorable experience from childhood | When I was 7, I went fishing for the first time...  | Human  |\n| Describe a memorable experience from childhood | Childhood memories often hold a special place...    | AI     |\n| What are your thoughts on remote work?         | Honestly, I love working from home. No commute...   | Human  |"
  },
  {
    "objectID": "posts/detecting-ai-text.html#time-to-apply-data-science-techniques-advanced-reading",
    "href": "posts/detecting-ai-text.html#time-to-apply-data-science-techniques-advanced-reading",
    "title": "Searching for Authentic Writing in a World of AI-Generated Text",
    "section": "Time to Apply Data Science Techniques! (Advanced Reading)",
    "text": "Time to Apply Data Science Techniques! (Advanced Reading)\nAfter collecting all the text samples (from humans and ChatGPT), we will create a data science model to identify whether the text is written by a human or AI. A model is simply a mathematical formula that learns from our examples (the dataset) and makes predictions on new data. To build this model, we look for patterns in the text such as: sentence length, word frequency, punctuation frequency, and diversity of vocabulary. When we combine multiple clues like these, they can start to reveal if the text was written by a human or AI.\nTo create (or train) a model, we need to first transform the raw text into numerical features through a process called feature engineering. This involves extracting measurable characteristics (the patterns we mentioned earlier) from each response. Once we have these features, we can feed them into a classification model and measure how well it performs using a metric called accuracy. The higher the accuracy, the better our model is at detecting if a text was written by a human or AI.\nThe classification model that we use for this task is called logistic regression. The goal of logistic regression is to estimate the probability that a given piece of text belongs to one of two categories: Human or AI. The model takes numerical features extracted from the text and combines them using learned weights. These weights determine how strongly each feature contributes to the final prediction. The output of the model is a score between 0 and 1, which can be interpreted as the probability that the text is AI-generated. For example, a score close to 1 suggests the text is more likely written by AI, while a score close to 0 suggests more human writing. In practice, this can be implemented using a standard machine learning library such as scikit-learn, where a model can be trained using code like LogisticRegression().fit(X_train, y_train). We can then evaluate how well the model performs by comparing its predictions to the known ground truth labels and calculating accuracy. This allows us to quantify how effective logistic regression is at distinguishing between human-written and AI-generated text based on the features we selected.\nFinally, we can test our model on new, unseen data to see how well it generalizes. By holding out a portion of our dataset during training, we can simulate real-world conditions where the model encounters text it has never seen before. If the model performs well on this test set, we can be more confident that it will accurately classify new responses in practice."
  },
  {
    "objectID": "posts/detecting-ai-text.html#concluding-remarks",
    "href": "posts/detecting-ai-text.html#concluding-remarks",
    "title": "Searching for Authentic Writing in a World of AI-Generated Text",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThis project aims to see if machines can help us identify text written by AI or not. However it is important to consider that AI systems are also constantly evolving (for example, by introducing the “human touch” to artificially-generated text). Limitations of using machines to detect AI-generated text need to be explored as humans also evolve their writing in response to AI writing.\n\nSome More Final Thoughts…\nAs I was writing this blog post, there were several moments where I wanted to use the em-dash. However, the em-dash has been getting a lot of negative attention lately. Because ChatGPT is known to use a lot of em-dashes in its writing, humans have been wrongly accused of using AI-assistance simply just by having em-dashes in their essay. This is just one small example of how humans are also changing the way they write in the age of AI. Perhaps one day the overlap between human and AI writing will become so blurry that it becomes impossible to tell them apart. The real question then might not be who wrote something, but whether it resonates with us as readers."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Johnson and on this blog I will post my tiny data science tinkerings and musings. I am currently studying in the Masters of Data Science program at the University of British Columbia."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tiny Data Stories",
    "section": "",
    "text": "Searching for Authentic Writing in a World of AI-Generated Text\n\n\n\n\n\n\nai\n\n\nchatgpt\n\n\ntext\n\n\nclassification\n\n\n\nClassifying Text with Machine Learning.\n\n\n\n\n\nJan 16, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nScores from the Kendama World Cup 2025\n\n\n\n\n\n\nvisualization\n\n\ndistribution\n\n\nkendama\n\n\n\nVisualizing the score distribution.\n\n\n\n\n\nNov 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHello, Tiny Data World\n\n\n\n\n\n\nintro\n\n\nmeta\n\n\n\nKicking off this blog with a fruity dataset.\n\n\n\n\n\nNov 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello-tiny-data-world.html",
    "href": "posts/hello-tiny-data-world.html",
    "title": "Hello, Tiny Data World",
    "section": "",
    "text": "This is my very first tiny data story.\nIt’s the weekend, end of 1st week of Block 3 at UBC MDS. All my assignments that were due at 6pm have been submitted. There is a small window of a break before we get to Monday and it all starts over. Anyway, I’ve been meaning to start a tiny blog (micro blog?) like this for a while, but never got to it, so now I’m finally here. You’re reading this because I finally found some time to put it all together.\nThe whole point of this tiny blog is to get me to practice looking at small bits and pieces of data regularly. Every now and then I come across some interesting small datasets, or I personally have some interesting small datasets from my own life that I’m interested in, and I want to try to use code and data analysis to answer some questions. So ya, that’s the point of this… micro blog. Let’s call it that for now.\nRight now, the goal is just to make sure everything works: the blog, the code, and the render. Everything should me more or less: load data, do analysis, add commentary, push blog post, and let quarto publish automatically. I don’t want to deal with the details of managing a blog site, so that’s why I’m letting quarto and github power all of this.\nWell, let’s start with our very first tiny data set just to make sure things are running as expected. Here I’m going to use python pandas and just create a small table of some of my favourite fruits. That is all, stay tuned for more!\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"fruit\": [\"apple\", \"banana\", \"kiwi\", \"guava\", \"mango\", \"pear\"],\n    \"rating\": [8, 6, 8, 9, 8, 7]\n})\ndf\n\n\n\n\n\n\n\n\nfruit\nrating\n\n\n\n\n0\napple\n8\n\n\n1\nbanana\n6\n\n\n2\nkiwi\n8\n\n\n3\nguava\n9\n\n\n4\nmango\n8\n\n\n5\npear\n7"
  }
]