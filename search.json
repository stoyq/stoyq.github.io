[
  {
    "objectID": "posts/kendama-cup-scores.html",
    "href": "posts/kendama-cup-scores.html",
    "title": "Scores from the Kendama World Cup 2025",
    "section": "",
    "text": "My friend recently showed me the final scores from the Kendama World Cup 2025:\n\n\n\nWorld Cup 2025 Final Scores\n\n\nI decided to plot out the distribution of scores.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nscores = [\n    2043, 1622, 1457, 1448, 1403, 1395, 1371, 1347, 1285, 1277,\n    1206, 1205, 1159, 1113, 1063, 1062, 1008, 1000, 1000, 956,\n    948, 939, 938, 924, 917, 911, 910, 885, 872, 869, 820, 797,\n    772, 729, 703, 666, 665, 662, 578, 536, 485, 472, 452, 340,\n    306, 273, 82\n]\n\ndf = pd.DataFrame({'score': scores})\n\nplt.figure(figsize=(10,6))\nplt.hist(df['score'], bins=10, edgecolor='black')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.title('Kendama World Cup 2025 Scores Histogram')\nplt.show()\n\n\n\n\n\n\n\n\nTo my surprise the distribution looks bell-shaped, with a very obvious peak at 1000! So while the table form is useful to look up player names and scores, the histogram is even more insightful. I did not expect scores from a competition to also be distributed with such a familar shape. What steps can I take next to see if the scores are normally distributed? Can I apply Central Limit Theorem to make any inferences?"
  },
  {
    "objectID": "posts/detecting-ai-text.html",
    "href": "posts/detecting-ai-text.html",
    "title": "Searching for Authentic Writing in a World of AI-Generated Text",
    "section": "",
    "text": "A human using a magnifying glass to distinguish between human-written and AI-generated text\n\n\nIn the past year, the use of generative AI tools such as ChatGPT has exploded in popularity. ChatGPT can be used to help people write emails, essays, summaries, and creative stories almost instantly. With more people using such tools, more people are wanting more authentic text written by real humans. So an important question has come up: can we distinguish between human-written and AI-generated text? There is plenty of research done in this area by (Fraser 2025; Georgiou 2025; Fiedler et al. 2025).\nThis project looks to answer that question using data science techniques. It is important to define the problem, because the term “AI-generated” means something different to different people. If a human uses AI to generate text (AI-assisted), does that still count as human-written? Should there be 3 different categories (human-written, AI-assisted, and AI-generated)? And if so, how much assistance can a human get from AI before the text is classified as AI-generated? For this project, we will keep it simple. We will compare text written completely by humans (unassisted by AI) with text written by AI (produced by generative tools such as ChatGPT).\nTo conduct this study, we will collect written samples from English speakers with different ages and educational backgrounds. Everyone will respond to the same prompt. Then we will also have ChatGPT respond to the same prompt. This way, we get two comparable sets of responses, and we know the ground truth.\nAfter collecting all the text samples (from humans and ChatGPT), we will use data science techniques to identify whether the text is written by a human or AI. We can measure the accuracy of our technique by simply comparing it to the ground truth. We aim to improve the technique to correctly classify the text as human-written or AI-generated. Some of the things we can use in our classification methodology include looking for patterns in the text such as: sentence length, word frequency, complexity and diversity of grammar and vocabulary. When we look at multiple clues, they can start to reveal if the text is written by human or AI.\nWe use logistic regression for this task. Basically the patterns we are looking for are the weights, and they form a score between 0 and 1. Higher scores suggest the text is more likely generated by AI, while lower scores suggest more human writing. We can test how accurate our machine learning model is by seeing how many correct identifications it produced.\nThis project aims to see if machines can help us identify text written by AI or not. However it is important to consider that AI systems are constantly evolving, and introducing the “human touch” to artificially-generated text. Limitations of using machines to detect AI-generated text need to be explored as humans evolve their writing in response to AI writing.\n\n\n\n\nReferences\n\nFiedler, Andreas et al. 2025. “Do Humans Identify AI-Generated Text Better Than Machines?” Computers in Human Behavior. https://www.sciencedirect.com/science/article/pii/S1477388025000131.\n\n\nFraser, Kathleen C. 2025. “Detecting AI-Generated Text: Methods, Accuracy, and Challenges.” Journal of Artificial Intelligence Research. https://www.jair.org/index.php/jair/article/view/16665.\n\n\nGeorgiou, George P. 2025. “Differentiating Between Human-Written and AI-Generated Text.” Information 16 (11). https://www.mdpi.com/2078-2489/16/11/979."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Johnson and on this blog I will post my tiny data science tinkerings and musings. I am currently studying in the Masters of Data Science program at the University of British Columbia."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tiny Data Stories",
    "section": "",
    "text": "Searching for Authentic Writing in a World of AI-Generated Text\n\n\n\n\n\n\nai\n\n\nchatgpt\n\n\ntext\n\n\nclassification\n\n\n\nClassifying Text with Machine Learning.\n\n\n\n\n\nJan 13, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nScores from the Kendama World Cup 2025\n\n\n\n\n\n\nvisualization\n\n\ndistribution\n\n\nkendama\n\n\n\nVisualizing the score distribution.\n\n\n\n\n\nNov 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHello, Tiny Data World\n\n\n\n\n\n\nintro\n\n\nmeta\n\n\n\nKicking off this blog with a fruity dataset.\n\n\n\n\n\nNov 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello-tiny-data-world.html",
    "href": "posts/hello-tiny-data-world.html",
    "title": "Hello, Tiny Data World",
    "section": "",
    "text": "This is my very first tiny data story.\nIt’s the weekend, end of 1st week of Block 3 at UBC MDS. All my assignments that were due at 6pm have been submitted. There is a small window of a break before we get to Monday and it all starts over. Anyway, I’ve been meaning to start a tiny blog (micro blog?) like this for a while, but never got to it, so now I’m finally here. You’re reading this because I finally found some time to put it all together.\nThe whole point of this tiny blog is to get me to practice looking at small bits and pieces of data regularly. Every now and then I come across some interesting small datasets, or I personally have some interesting small datasets from my own life that I’m interested in, and I want to try to use code and data analysis to answer some questions. So ya, that’s the point of this… micro blog. Let’s call it that for now.\nRight now, the goal is just to make sure everything works: the blog, the code, and the render. Everything should me more or less: load data, do analysis, add commentary, push blog post, and let quarto publish automatically. I don’t want to deal with the details of managing a blog site, so that’s why I’m letting quarto and github power all of this.\nWell, let’s start with our very first tiny data set just to make sure things are running as expected. Here I’m going to use python pandas and just create a small table of some of my favourite fruits. That is all, stay tuned for more!\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"fruit\": [\"apple\", \"banana\", \"kiwi\", \"guava\", \"mango\", \"pear\"],\n    \"rating\": [8, 6, 8, 9, 8, 7]\n})\ndf\n\n\n\n\n\n\n\n\nfruit\nrating\n\n\n\n\n0\napple\n8\n\n\n1\nbanana\n6\n\n\n2\nkiwi\n8\n\n\n3\nguava\n9\n\n\n4\nmango\n8\n\n\n5\npear\n7"
  }
]